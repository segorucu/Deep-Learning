# -*- coding: utf-8 -*-
"""Hateful-Meme_Text.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16xxslaXr32dxRGWYuj7b2Oic8GBx-sqq
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive

drive.mount("/content/drive")

# %cd "/content/drive/MyDrive/HatefulMeme/data"

!pip install transformers

import numpy as np 
import pandas as pd
import torch,json, os
from PIL import Image
import matplotlib.pyplot as plt
import torchvision
from torchvision import datasets, models, transforms
import torchvision.transforms.functional as F
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
from matplotlib.ticker import MaxNLocator
import torch
from transformers import BertTokenizer, BertModel
from torch.optim import Adam
from tqdm import tqdm

dev_path = "dev.jsonl"
train_path = "train.jsonl"
test_path = "test.jsonl"
img_path = "/img/"

# transform = transforms.Compose([transforms.ToTensor()])
Transf = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])

class HatefulMemesDataset(torch.utils.data.Dataset):
    def __init__(self, data_path):
        self.data = [json.loads(l) for l in open(data_path)]
        self.data_dir = os.path.dirname(data_path)
            
    def __getitem__(self, index: int):
        # Load images on the fly.
        try:
          image = Transf(Image.open(os.path.join(self.data_dir, self.data[index]["img"])).convert("RGB"))
          text = [tokenizer(text, padding='max_length', max_length = 512, truncation=True, return_tensors="pt") for text in self.data[index]["text"]]
          label = self.data[index]["label"]
        except FileNotFoundError as e:
          image = Transf(Image.open(os.path.join(self.data_dir, self.data[1]["img"])).convert("RGB"))
          text = [tokenizer(text, padding='max_length', max_length = 512, truncation=True, return_tensors="pt") for text in self.data[index]["text"]]
          label = self.data[1]["label"]
        
        # return image, text, label
        return {'images':image, 'text':text, 'labels':label}
        # return image, label

    def load_image_only(self, index: int):
        image = Image.open(os.path.join(self.data_dir, self.data[index]["img"])).convert("RGB")
        return image
    
    def get_label(self, index: int):
        label = self.data[index]["label"]
        return label
    
    def get_test_item(self, index: int):
        # Load images on the fly.
        image = Image.open(os.path.join(self.data_dir, self.data[index]["img"])).convert("RGB")
        text = self.data[index]["text"]
        return image, text
   
    def __len__(self):
        return len(self.data)


train_data = HatefulMemesDataset(train_path)
val_data = HatefulMemesDataset(dev_path)
test_data = HatefulMemesDataset(test_path)

print('Data size of training data: %d samples' % len(train_data))
print('Data size of validation data: %d samples' % len(val_data))
print('Data size of test data: %d samples' % len(test_data))

train_loader = DataLoader(train_data, batch_size=512, shuffle=False)
valid_loader = DataLoader(val_data, batch_size=128, shuffle=False)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device

model = models.resnet50(pretrained=True).to(device)
    
for param in model.parameters():
    param.requires_grad = False   
    
model.fc = nn.Sequential(
               nn.Linear(2048, 128),
               nn.ReLU(inplace=True),
               nn.Linear(128, 2)).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.fc.parameters(), lr=0.001)

def fit(epochs,model,optimizer,criteria):
  train_loss = []
  train_accuracy = []
  valid_loss = []
  valid_accuracy = []

  for epoch in range(epochs):
      training_loss = 0.0
      validation_loss = 0.0
      correct = 0.0
      total = 0
      print('{}/{} Epochs'.format(epoch+1,epochs))
      
      model.train()
      for batch_idx,d in enumerate(train_loader):
          data = d['images'].to(device)
          target = d['labels'].to(device)
          
          optimizer.zero_grad()
          output = model(data)
          loss = criteria(output,target)
          loss.backward()
          optimizer.step()
          
          training_loss = training_loss + ((1/(batch_idx+1))*(loss.data-training_loss))
          if batch_idx%20==0:
              print('Training loss {}'.format(training_loss))
          pred = output.data.max(1,keepdim=True)[1]
          correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())
          # correct += (pred == target).sum()
          total +=data.size(0)
          print('Accuracy on batch {} on Training is {}'.format(batch_idx,(100*correct/total)))
        
      train_loss.append(training_loss)
      train_accuracy.append(100 * correct / total)

      model.eval()
      for batch_idx ,d in enumerate(valid_loader):
          data = d['images'].to(device)
          target = d['labels'].to(device)
          
          output = model(data)
          loss = criteria(output,target)
          
          validation_loss = validation_loss +((1/(batch_idx+1))*(loss.data-validation_loss))
          if batch_idx%20==0:
              print('Validation_loss {}'.format(validation_loss))
          pred = output.data.max(1,keepdim=True)[1]
          correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())
          # correct += (pred == target).sum()
          total+=data.size(0)
          print('Validation Accuracy on Batch {} is {}'.format(batch_idx,(100*correct/total)))

      valid_loss.append(validation_loss)
      valid_accuracy.append(100 * correct / total)
          
  return model, train_loss, valid_loss, train_accuracy, valid_accuracy

n_epochs = 10
model, train_loss, valid_loss, train_accuracy, valid_accuracy =  fit(n_epochs,model,optimizer,criterion)

class BertClassifier(nn.Module):

    def __init__(self, dropout=0.5):

        super(BertClassifier, self).__init__()

        self.bert = BertModel.from_pretrained('bert-base-cased')
        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(768, 2)
        self.relu = nn.ReLU()

    def forward(self, input_id, mask):

        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)
        dropout_output = self.dropout(pooled_output)
        linear_output = self.linear(dropout_output)
        final_layer = self.relu(linear_output)

        return final_layer

model_bert = BertClassifier()

train, val = Dataset(train_data), Dataset(val_data)

# train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)
# val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)

use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")

criterion = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr= learning_rate)

if use_cuda:
  model = model.cuda()
  criterion = criterion.cuda()

def train(model, train_data, val_data, learning_rate, epochs):

    for epoch_num in range(epochs):

            total_acc_train = 0
            total_loss_train = 0

            for train_input, train_label in tqdm(train_dataloader):

                train_label = train_label.to(device)
                mask = train_input['attention_mask'].to(device)
                input_id = train_input['input_ids'].squeeze(1).to(device)

                output = model(input_id, mask)
                
                batch_loss = criterion(output, train_label.long())
                total_loss_train += batch_loss.item()
                
                acc = (output.argmax(dim=1) == train_label).sum().item()
                total_acc_train += acc

                model.zero_grad()
                batch_loss.backward()
                optimizer.step()
            
            total_acc_val = 0
            total_loss_val = 0

            with torch.no_grad():

                for val_input, val_label in val_dataloader:

                    val_label = val_label.to(device)
                    mask = val_input['attention_mask'].to(device)
                    input_id = val_input['input_ids'].squeeze(1).to(device)

                    output = model(input_id, mask)

                    batch_loss = criterion(output, val_label.long())
                    total_loss_val += batch_loss.item()
                    
                    acc = (output.argmax(dim=1) == val_label).sum().item()
                    total_acc_val += acc
            
            print(
                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} | Train Accuracy: {total_acc_train / len(train_data): .3f} | Val Loss: {total_loss_val / len(val_data): .3f} | Val Accuracy: {total_acc_val / len(val_data): .3f}')

EPOCHS = 5
LR = 1e-3
              
train(model_bert, df_train, df_val, LR, EPOCHS)