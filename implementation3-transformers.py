# -*- coding: utf-8 -*-
"""Copy of Veliogluend2end_process.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15rx-rruOxj5wPPyk95r0OHj9NS71VsGU

# <font color='Aqua'> <b> Team HateDetectron - Phase-2 Submissions </b> </font>

---
This notebook documents *everything* **-all the code-** done for the Hateful Memes challenge. If you are only interested in reproducing the challenge submissions, please head over to the [reproduce-submissions notebook](https://colab.research.google.com/drive/1kAYFd50XvFnLO-k9FU9iLM21J8djTo-Q?usp=sharing) where the fine-tuned models are loaded and the predictions for Phase-2 test data are gathered. 


---

**Author:**\
<font color='Wheat'>
    <b>
        Riza Velioglu
    </b>
</font>

**Contact:**

<center>
<a href="http://rizavelioglu.github.io/"><img src="https://drive.google.com/uc?id=1SWc-ryZf7xxZ_g7AdU_vn2Y451IcCisw" width="200"></a>

[Webpage](http://rizavelioglu.github.io/)
</center>

# Table of Contents

<details><summary>
<font color='Tan'> I. Installation of MMF & dependencies </font></summary>

- Install MMF from source
</details>

<details><summary>
<font color='Tan'> II. Download the datasets & convert them into MMF format </font></summary>


</details>


<details><summary>
<font color='Tan'> III. Feature Extraction </font></summary>

</details>

<details><summary>
<font color='Tan'> IV. Fine-tuning pre-trained VisualBERT models on Hateful Memes </font></summary>

</details>


<details><summary>
<font color='Tan'> V. Generate predictions for the Challenge (`test_unseen.jsonl`) </font></summary>

</details>


<details><summary>
<font color='Tan'> Extras </font></summary>

</details>

<details><summary>
<font color='Tan'> Summary & FAQ </font></summary>

</details>

## <font color='magenta'> <b> I. Installation of MMF & dependencies </b> </font>
"""

from google.colab import drive 
drive.mount("/content/drive", force_remount=True)

"""Please set your `$HOME` directory.\
**e.g.** For *Linux* users it can be: `"/home"`,\
For *Colab* it would be: `"/content"`
"""

import os
home = "/content"
os.chdir(home)
os.getcwd()

# Install specified versions of `torch` and `torchvision`, before installing mmf (causes an issue)
!pip install torch==1.6.0 torchvision==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html

"""#### *Install MMF from source* 

"""

# Clone the following repo where mmf does not install default image features, 
# since we will use our own features
!git clone --branch no_feats --config core.symlinks=true https://github.com/rizavelioglu/mmf.git

os.chdir(os.path.join(home, "mmf"))

!pip install --editable .

"""---
## <font color='magenta'> <b> II. Download the datasets & convert them into *MMF* format </b> </font> <font color='red'><b> --Action required!-- </b></font>

### <font color='Orchid'> <b> Hateful Memes-Phase 2 dataset </b> </font>

Please download the `Hateful Memes Dataset` from the official challenge webpage: https://hatefulmemeschallenge.com/#download

After filling the form the `hateful_memes.zip` file will be downloaded, which includes all the required data including images. Please define the variable `PATH_TO_ZIP_FILE` in the following code cell which stores the full path of the downloaded `.zip` file:
"""

PATH_TO_ZIP_FILE = "/content/drive/MyDrive/hateful_memes.zip"
!cp $PATH_TO_ZIP_FILE /content/mmf/

# Add the mmf folder to Python Path
os.environ['PYTHONPATH'] += ":/content/mmf/"

!mmf_convert_hm --zip_file="hateful_memes.zip"

# Check how many images we have in total
!ls /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/ | wc -l

"""That means there are `12.140` **'uniquely named'** images in total and you might recall that the sizes of each set was the following:

- `|train.jsonl| = 8.500`
- `|dev_seen.jsonl| = 500`
- `|dev_unseen.jsonl| = 540`
- `|test_seen.jsonl| = 1.000`
- `|test_unseen.jsonl| = 2.000`

Well, this makes `8.500 + 500 + 540 + 1.000 + 2.000 = 12.540` in total. \
> *Is there something wrong?*\
> **TL;DR:** Nope. Some images in `dev_seen` are used in `dev_unseen`, too. To be specific, they have `400` common images. Hence, in total we have `12.540 - 400 = 12.140` *'unique'* images.\
See <font color='orange'> <b> Extras </b> </font> --> <font color='Gold'><b> Number of 'unique' (based on file names) images </b></font> at the end of this script to see the explanation in detail.
"""

# Free up the disk by removing .zip, .tar files
!rm -rf /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/hateful_memes.zip
!rm -rf $home/mmf/hateful_memes.zip

"""### <font color='Orchid'> <b> Memotion dataset </b> </font>

There are 2 options for downloading the dataset: 
1. download the dataset (a `.zip` file) using `Kaggle API`\
OR
2. download the dataset (a `.zip` file) from [Kaggle](https://www.kaggle.com/williamscott701/memotion-dataset-7k) directly, **(<font color='red' >preferred </font> if you're not familiar with Kaggle API)**

#### <font color='Thistle'> <b> 1. Download Memotion dataset using `Kaggle API` </b> </font>

Check out the official documentation to get more information on Kaggle API and how to create a Kaggle API Key:
- [Link#1](https://github.com/Kaggle/kaggle-api#api-credentials)
- [Link#2](https://www.kaggle.com/docs/api)

The API Key is stored in a file named `kaggle.jsonl`, which has the folowing line inside: 
`{"username":"your_user_name","key":"some_values_here"}`

> Upload the `kaggle.json` file to your `$HOME` directory and run the following cell.
"""

# Install kaggle library
!pip install -q kaggle
# Create a directory where API key will be stored
!mkdir -p ~/.kaggle
# Move the API key to where Kaggle expects it to be
!mv $home/kaggle.json ~/.kaggle/
# Give according rights to the file
!chmod 600 /root/.kaggle/kaggle.json
# Finally, download the dataset (.zip file)
!kaggle datasets download -d williamscott701/memotion-dataset-7k
# Unzip the data 
!unzip -qq memotion-dataset-7k.zip -d $home/

"""#### <font color='Thistle'> <b> 2. Download Memotion dataset directly from [Kaggle](https://www.kaggle.com/williamscott701/memotion-dataset-7k) </b> </font>

Download the dataset and put the `.zip` file into your `$HOME` directory and then run the following cell:
"""

# Unzip the data 
# !unzip memotion-dataset-7k.zip -d $home/

"""#### <font color='Thistle'> <b> Labeling Memotion Dataset </b> </font>

We have added `Memotion Dataset` to `Hateful Memes Dataset` and fine-tuned some models on the *aggregated* data. But there was no significant improvement seen neither on the `ROC-AUC score`, nor on the `accuracy`. We then discovered that the dataset is *horribly* labeled. Therefore, one needs to label the dataset.

So we went through the dataset and cherry-picked the memes that would be suitable for the challenge, considering the idea of `Hateful Memes Challenge`.

The following cell can be run to clone a repository which includes helpful scripts for the project such as; a script for labeling the `Memotion Dataset` and saving the data in the same format as the `Hateful Memes Dataset`.
"""

os.chdir(home)
!git clone https://github.com/rizavelioglu/hateful_memes-hate_detectron.git

"""Labeling the dataset is not necessary for reproducing our results but one can check out the [labeling script](https://github.com/rizavelioglu/hateful_memes-hate_detectron/tree/main/utils/label_memotion.py) and execute the following line of code to run the script and see how the labeling is done.

```
# Start labeling Memotion dataset and save it at the end
%run $home/hateful_memes-hate_detectron/utils/label_memotion.py --home $home
```

---
> In total, we have labeled $328$ memes.\
Check out the following file to find the ones we labeled: [/hateful_memes-hate_detectron/utils/label_memotion.jsonl](https://github.com/rizavelioglu/hateful_memes-hate_detectron/tree/main/utils/label_memotion.jsonl)

Next, we move those labeled images from `Memotion Dataset` into the same folder where the images from `Hateful Memes Dataset` are, so that when the image features are being extracted all the images are inside the same folder.

"""

import pandas as pd
# read the .jsonl file and get the img column
labeled_memo_samples = pd.read_json(os.path.join(home, "hateful_memes-hate_detectron/utils/label_memotion.jsonl"), lines=True)['img']
# parse the img entries and get the image names
labeled_memo_samples = [i.split('/')[1] for i in list(labeled_memo_samples)]

img_dir = os.path.join(home, f"memotion_dataset_7k/images/")
for img in labeled_memo_samples:
    os.rename(f"{img_dir+img}", f"/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/{img}")

# Check how many images we have in total
!ls /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/ | wc -l

"""### <font color='Orchid'> <b> Merging the two datasets to get a larger training data</b> </font>

Simply execute the following cell which concatanates;
- Labeled Memotion dataset,
- Hateful Memes' training data, and
- 100 images from `dev_seen.jsonl` that are not in `dev_unseen.jsonl`

and generates `train_v10.jsonl`, which will be used for fine-tuning.
"""

!python $home/hateful_memes-hate_detectron/utils/concat_memotion-hm.py --home $home

"""---
## <font color='magenta'> <b> III. Feature Extraction </b> </font>

### <font color='lightgreen'> <b> Extract image features using [`mmf/tools/scripts/features/extract_features_vmb.py`](https://github.com/facebookresearch/mmf/blob/master/tools/scripts/features/extract_features_vmb.py) </b> </font>

#### Install packages & repos
"""

import os
os.chdir(home)
!git clone https://gitlab.com/vedanuj/vqa-maskrcnn-benchmark

!pip install ninja yacs cython matplotlib

os.chdir(os.path.join(home, "vqa-maskrcnn-benchmark"))
!rm -rf build
!python setup.py build develop

"""#### Extract!"""

# !wget https://dl.fbaipublicfiles.com/pythia/detectron_model/FAST_RCNN_MLP_DIM2048_FPN_DIM512.pkl
# !wget https://dl.fbaipublicfiles.com/pythia/detectron_model/e2e_faster_rcnn_X-101-64x4d-FPN_1x_MLP_2048_FPN_512.yaml
os.chdir(os.path.join(home, "mmf/tools/scripts/features/"))
out_folder = os.path.join(home, "features/")

!python extract_features_vmb.py --config_file "https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model_x152.yaml" \
                                --model_name "X-152" \
                                --output_folder $out_folder \
                                --image_dir "/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/" \
                                --num_features 100 \
                                # --exclude_list "/content/exclude.txt"
                                # --feature_name "fc6" \
                                # --confidence_threshold 0. \

"""---
## <font color='magenta'> <b> IV. Fine-tuning pre-trained VisualBERT models on Hateful Memes </b> </font>

```
# This is formatted as code
```

### <font color='Violet'> <b> Train Transformer </b> </font>
"""

"""
Uncomment it if needed
"""

os.chdir(home)
# Define where image features are
feats_dir = os.path.join(home, "/content/drive/MyDrive/features")
# Define where train.jsonl is
train_dir = os.path.join(home, "train_v10.jsonl")

!mmf_run config="projects/mmf_transformer/configs/hateful_memes/defaults.yaml" \
        model="mmf_transformer" \
        dataset=hateful_memes \
        run_type=train_val \
        checkpoint.max_to_keep=1 \
        checkpoint.resume_zoo=visual_bert.pretrained.cc.full \
        training.tensorboard=True \
        training.checkpoint_interval=50 \
        training.evaluation_interval=50 \
        training.max_updates=1000 \
        training.log_interval=100 \
        dataset_config.hateful_memes.max_features=100 \
        dataset_config.hateful_memes.annotations.train[0]=$train_dir \
        dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \
        dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \
        dataset_config.hateful_memes.features.train[0]=$feats_dir \
        dataset_config.hateful_memes.features.val[0]=$feats_dir \
        dataset_config.hateful_memes.features.test[0]=$feats_dir \
        training.lr_ratio=0.6 \
        training.warmup_factor=0.2 \
        training.use_warmup=True \
        training.batch_size=32 \
        optimizer.params.lr=5.0e-05 \
        scheduler.params.num_warmup_steps=250 \
        scheduler.type=warmup_cosine \
        scheduler.params.num_training_steps=5000 #\
        #env.save_dir=/content/drive/MyDrive/sub3 \
        #env.tensorboard_logdir=/content/drive/MyDrive/logs/fit/sub3

"""#Validation transformer"""

"""
Uncomment it if needed
"""

os.chdir(home)
# Define where image features are
feats_dir = os.path.join(home, "/content/drive/MyDrive/features")
# Define where train.jsonl is
train_dir = os.path.join(home, "train_v10.jsonl")
ckpt_dir = os.path.join(home, "/content/save/best.ckpt")

!mmf_run config="projects/mmf_transformer/configs/hateful_memes/defaults.yaml" \
        model="mmf_transformer" \
        dataset=hateful_memes \
        run_type=test \
        checkpoint.resume_file=$ckpt_dir \
        dataset_config.hateful_memes.max_features=100 \
        dataset_config.hateful_memes.annotations.train[0]=$train_dir \
        dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \
        dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \
        dataset_config.hateful_memes.features.train[0]=$feats_dir \
        dataset_config.hateful_memes.features.val[0]=$feats_dir \
        dataset_config.hateful_memes.features.test[0]=$feats_dir

!pip install tensorboard==2.8.0

"""##### **Visualize losses/accuracy via Tensorboard**"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/drive/MyDrive/logs/fit

"""### <font color='Violet'> <b> VILBERT </b> </font>"""

"""
Uncomment it if needed
"""

os.chdir(home)
# Define where image features are
feats_dir = os.path.join(home, "/content/drive/MyDrive/features")
# Define where train.jsonl is
train_dir = os.path.join(home, "train_v10.jsonl")


!mmf_run config="projects/vilbert/configs/hateful_memes/defaults.yaml" \
        model="vilbert" \
        dataset=hateful_memes \
        run_type=train_val \
        checkpoint.max_to_keep=1 \
        checkpoint.resume_zoo=vilbert.pretrained.cc.full \
        training.tensorboard=True \
        training.checkpoint_interval=50 \
        training.evaluation_interval=50 \
        training.max_updates=500 \
        training.log_interval=100 \
        dataset_config.hateful_memes.max_features=100 \
        dataset_config.hateful_memes.annotations.train[0]=$train_dir \
        dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \
        dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \
        dataset_config.hateful_memes.features.train[0]=$feats_dir \
        dataset_config.hateful_memes.features.val[0]=$feats_dir \
        dataset_config.hateful_memes.features.test[0]=$feats_dir \
        training.lr_ratio=0.6 \
        training.use_warmup=True \
        training.batch_size=80 \
        optimizer.params.lr=5.0e-05 \
        scheduler.params.num_warmup_steps=500 \
        scheduler.type=warmup_cosine \
        scheduler.params.num_training_steps=5000 \
        env.save_dir=/content/drive/MyDrive/vilbert \
        env.tensorboard_logdir=logs/fit/vilbert

"""# MMBT"""

"""
Uncomment it if needed
"""

os.chdir(home)
# Define where image features are
feats_dir = os.path.join(home, "/content/drive/MyDrive/features")
# Define where train.jsonl is
train_dir = os.path.join(home, "train_v10.jsonl")

!mmf_run config="projects/mmbt/configs/hateful_memes/defaults.yaml" \
        model="mmbt" \
        dataset=hateful_memes \
        run_type=train_val \
        checkpoint.max_to_keep=1 \
        #checkpoint.resume_zoo=mmbt.pretrained.cc.full \
        checkpoint.resume_pretrained=True \
        training.tensorboard=True \
        training.checkpoint_interval=50 \
        training.evaluation_interval=50 \
        training.max_updates=500 \
        training.log_interval=100 \
        dataset_config.hateful_memes.max_features=100 \
        dataset_config.hateful_memes.annotations.train[0]=$train_dir \
        dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \
        dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \
        dataset_config.hateful_memes.features.train[0]=$feats_dir \
        dataset_config.hateful_memes.features.val[0]=$feats_dir \
        dataset_config.hateful_memes.features.test[0]=$feats_dir \
        training.lr_ratio=0.6 \
        training.warmup_factor=0.2 \
        training.use_warmup=True \
        training.batch_size=32 \
        optimizer.params.lr=5.0e-05 \
        scheduler.params.num_warmup_steps=250 \
        scheduler.type=warmup_cosine \
        scheduler.params.num_training_steps=5000 #\
        env.save_dir=/content/save/mmbt \
        env.tensorboard_logdir=/content/save/logs/fit/mmbt

"""
Uncomment it if needed
"""

os.chdir(home)
# Define where image features are
feats_dir = os.path.join(home, "/content/drive/MyDrive/features")
# Define where train.jsonl is
train_dir = os.path.join(home, "train_v10.jsonl")
mmbt_dir = os.path.join(home, "/content/save/mmbt_final.pth")

!mmf_run config="projects/mmbt/configs/hateful_memes/defaults.yaml" \
        model="mmbt" \
        dataset=hateful_memes \
        run_type=test \
        checkpoint.resume_file=$mmbt_dir \
        dataset_config.hateful_memes.max_features=100 \
        dataset_config.hateful_memes.annotations.train[0]=$train_dir \
        dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \
        dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \
        dataset_config.hateful_memes.features.train[0]=$feats_dir \
        dataset_config.hateful_memes.features.val[0]=$feats_dir \
        dataset_config.hateful_memes.features.test[0]=$feats_dir

"""#submission 6"""

"""
Uncomment it if needed
"""

os.chdir(home)
# Define where image features are
feats_dir = os.path.join(home, "/content/drive/MyDrive/features")
# Define where train.jsonl is
train_dir = os.path.join(home, "train_v10.jsonl")

!mmf_run config="projects/others/cnn_lstm/hateful_memes/defaults.yaml" \
        model="cnn_lstm" \
        dataset=hateful_memes \
        run_type=train_val \
        checkpoint.max_to_keep=1 \
        checkpoint.resume_zoo=visual_bert.pretrained.cc.full \
        training.tensorboard=True \
        training.checkpoint_interval=50 \
        training.evaluation_interval=50 \
        training.max_updates=3500 \
        training.log_interval=100 \
        dataset_config.hateful_memes.max_features=100 \
        dataset_config.hateful_memes.annotations.train[0]=$train_dir \
        dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \
        dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \
        dataset_config.hateful_memes.features.train[0]=$feats_dir \
        dataset_config.hateful_memes.features.val[0]=$feats_dir \
        dataset_config.hateful_memes.features.test[0]=$feats_dir \
        training.lr_ratio=0.6 \
        training.use_warmup=True \
        training.batch_size=32 \
        optimizer.params.lr=5.0e-05 \
        scheduler.params.num_warmup_steps=500 \
        scheduler.type=warmup_cosine \
        scheduler.params.num_training_steps=5000 \
        env.save_dir=/content/drive/MyDrive/cnn_lstm \
        env.tensorboard_logdir=/content/drive/MyDrive/logs/fit/cnn_lstm

"""### <font color='Violet'> <b> Submission#3 </b> </font>"""

os.chdir(os.path.join(home, "hateful_memes-hate_detectron/hyperparameter_sweep"))
# Give rights to bash script to be executable
!chmod +x sweep.sh
# Define where image features are
feats_dir = os.path.join(home, "features")
# Define where train.jsonl is
train_dir = "hateful_memes/defaults/annotations/train.jsonl"
# os.path.join(home, "train_v10.jsonl")

# Start hyper-parameter search
!python sweep.py --home $home --feats_dir $feats_dir --train $train_dir

"""After a hyper-parameter search, we ended up having multiple models having different ROC-AUC scores on `dev_unseen` dataset. We sorted them by the ROC score and took all the models that have a ROC score of `0.76` or higher (the threshold is chosen arbitrarily). The following figure shows all the $27$ models and its ROC-scores, as well as its hyper-parameters ([see this document for all the model scores, 60+ models in total](https://docs.google.com/spreadsheets/d/11m2p7vNxHhZWumkFNvv6d94HcqG77DItRX7MuGfOtWA/edit?usp=sharing)).

<details><summary>
<font color='Tan'> Figure 1: ROC-AUC scores (on `dev_unseen`) of different VisualBERT models </font></summary>

<img src="https://drive.google.com/uc?id=10WUBnSO5L5O44c8WCxHRA_iFZudH73lF" width="1000"> 

</details>

Then, predictions are collected from each of the $27$ models and the `Majority Voting` technique is applied: the `class` of a data point is determined by the majority class.

> This technique is also known as: [Voting classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html), [Ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning), [Bootstrap Aggregating (BAGGING)](https://en.wikipedia.org/wiki/Bootstrap_aggregating#:~:text=Bootstrap%20aggregating%2C%20also%20called%20bagging,and%20helps%20to%20avoid%20overfitting.)

Please see [this document](https://drive.google.com/file/d/1vjUsMqaqjZdoNj0w989RX7GKkPod-CGo/view?usp=sharing) to see all the model predictions for `test_unseen` and how the technique is applied.

**Note:**  The `proba` column in the submission which stands for the probability of a data point belonging to a class is chosen to be the maximum probability among all of the $27$ models if $Class\ 1$, and the minumum if $Class\ 0$.
"""

while(1):
  pass

!python get_27_models.py

"""Generate submission for each model"""

!chmod +x ../utils/generate_submission.sh

from subprocess import call
os.chdir("majority_voting_models")
models = [i for i in os.listdir(".") if i.endswith(".ckpt")]

print(f"[INFO] Getting predictions for {len(models)} models! This might take long..")
for model in models:
    feats_dir = os.path.join(home, "features")
    # Execute the bash script which gets predictions for 'test_unseen' data
    rc = call(f"../../utils/generate_submission.sh {model} {feats_dir}", shell=True)

"""Apply majority voting"""

import numpy as np
import pandas as pd

# Store all the prediction folders
folders = [i for i in os.listdir("save/preds") if i.startswith("hateful_memes")]
preds = pd.DataFrame()

try:
    for folder in folders:
        pred = [i for i in os.listdir(f"save/preds/{folder}/reports/") if i.endswith(".csv")]
        pred = pd.read_csv(f"save/preds/{folder}/reports/{pred[0]}")
        preds = pd.concat([preds, pred], axis=1)
except:
    pass

# assert len(preds.columns) == 27*3

# Create 
submission = pred
np_df = np.asarray(preds)

for idx, row in enumerate(np_df[:,:]):
    probas = row[1::3]
    labels = row[2::3]

    if sum(labels) > 13:
        submission.loc[idx, 'label']=1
        submission.loc[idx, 'proba']=probas.max()    
    else:
        submission.loc[idx, 'label']=0
        submission.loc[idx, 'proba']=probas.min()

"""Sort the submission with regards to the submission template & save the final submission file to `submission#3.csv`"""

os.chdir(home)
# Download the Phase2 submission template
!wget -O submission_format_phase_2.csv  "https://drivendata-prod.s3.amazonaws.com/data/70/public/submission_format_phase_2.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARVBOBDCYVI2LMPSY%2F20201201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201201T023533Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=04330cf22c33f1817cac29509178d2c11a07d620e8237241c5088f4fd25df2b3"
template = pd.read_csv("submission_format_phase_2.csv")
# Sort the 'submission' file
submission = submission.set_index('id')
submission = submission.reindex(index=template['id'])
submission = submission.reset_index()
# Save submission file
submission.to_csv(f"{home}/submission#3.csv", index=False)

"""---
## <font color='magenta'> <b> V. Generate predictions for the Challenge (`test_unseen.jsonl`) </b> </font>

### <font color='Thistle'> <b> Submission#1 </b> </font>
"""

"""
Uncomment it if needed
"""

# os.chdir(home)
# # where checkpoint is
# ckpt_dir = os.path.join(home, "sub1/best.ckpt")
# feats_dir = os.path.join(home, "features/feats_hm")

# !mmf_predict config="projects/visual_bert/configs/hateful_memes/defaults.yaml" \
#     model="visual_bert" \
#     dataset=hateful_memes \
#     run_type=test \
#     checkpoint.resume_file=$ckpt_dir \
#     checkpoint.reset.optimizer=True \
#     dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \
#     dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \
#     dataset_config.hateful_memes.features.train[0]=$feats_dir \
#     dataset_config.hateful_memes.features.val[0]=$feats_dir \
#     dataset_config.hateful_memes.features.test[0]=$feats_dir \

"""### <font color='Thistle'> <b> Submission#2 </b> </font>"""

"""
Uncomment it if needed
"""

# os.chdir(home)
# # where checkpoint is
# ckpt_dir = os.path.join(home, "sub2/best.ckpt")
# feats_dir = os.path.join(home, "features/feats_hm")

# !mmf_predict config="projects/visual_bert/configs/hateful_memes/defaults.yaml" \
#     model="visual_bert" \
#     dataset=hateful_memes \
#     run_type=test \
#     checkpoint.resume_file=$ckpt_dir \
#     checkpoint.reset.optimizer=True \
#     dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \
#     dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \
#     dataset_config.hateful_memes.features.train[0]=$feats_dir \
#     dataset_config.hateful_memes.features.val[0]=$feats_dir \
#     dataset_config.hateful_memes.features.test[0]=$feats_dir \

"""### <font color='Thistle'> <b> Submission#3 </b> </font>

Please see Part <font color='magenta'> <b> IV. Fine-tuning pre-trained VisualBERT models on Hateful Memes </b> </font> to see how the submission is created.

--- 
## <font color='orange'> <b> Extras </b> </font>

### <font color='Gold'> <b> Image feature type conversion </b> </font>
Convert image features from `.npy` --> `.lmdb` and vice versa

You can also try to use the .npy files directly. Just point to the folder which contains those files in your config. lmdb is not a necessary requirement.

#### <font color='PaleGoldenrod'> <b> Convert .npy files to .lmdb </b> </font>
"""

import argparse
import glob
import os
import pickle

import lmdb
import numpy as np
import tqdm


class LMDBConversion():
    def __init__(self, features_folder, lmdb_path):
        self.features_folder = features_folder
        self.lmdb_path = lmdb_path

    def convert(self):
        env = lmdb.open(self.lmdb_path, map_size=1099511627776)
        id_list = []
        features = glob.glob(
            os.path.join(self.features_folder, "**", "*.npy"), recursive=True
        )


        with env.begin(write=True) as txn:
            for infile in tqdm.tqdm(features):
                reader = np.load(infile, allow_pickle=True)
                item = {}
                split = os.path.relpath(infile, self.features_folder).split(
                    ".npy"
                )[0]
                item["feature_path"] = split
                key = split.encode()
                id_list.append(key)

                item["features"] = reader.item().get("features")
                item["image_height"] = reader.item().get("image_height")
                item["image_width"] = reader.item().get("image_width")
                item["num_boxes"] = reader.item().get("num_boxes")
                item["objects"] = reader.item().get("objects")
                item["cls_prob"] = reader.item().get("cls_prob", None)
                item["bbox"] = reader.item().get("bbox")

                txn.put(key, pickle.dumps(item))

            txn.put(b"keys", pickle.dumps(id_list))

features_folder = '/content/features/'
lmdb_path = "/content/"
lmdb_converter = LMDBConversion(features_folder, lmdb_path)
lmdb_converter.convert()

"""#### <font color='PaleGoldenrod'> <b> Convert .lmdb to .npy </b> </font>
just to check if everything's okay
"""

features_folder = "/content/features_from_lmdb/"
lmdb_path = "/content/drive/MyDrive/"

def extract():
    os.makedirs(features_folder, exist_ok=True)
    env = lmdb.open(
        lmdb_path,
        max_readers=1,
        readonly=True,
        lock=False,
        readahead=False,
        meminit=False,
    )
    with env.begin(write=False) as txn:
        _image_ids = pickle.loads(txn.get(b"keys"))
        for img_id in tqdm.tqdm(_image_ids):
            item = pickle.loads(txn.get(img_id))
            img_id = img_id.decode("utf-8")
            
            tmp_dict = {
                "image_id"    : img_id,
                "bbox"        : item["bbox"],
                "num_boxes"   : item["num_boxes"],
                "image_height": item["image_height"],
                "image_width" : item["image_width"],
                "objects"     : item["objects"],
                "cls_prob"    : item["cls_prob"],
            }

            info_file_base_name = str(img_id) + "_info.npy"
            file_base_name = str(img_id) + ".npy"

            np.save(
                os.path.join(features_folder, file_base_name),
                item["features"],
            )
            np.save(
                os.path.join(features_folder, info_file_base_name),
                tmp_dict,
            )

extract()

data = np.load("/content/features_from_lmdb/01243.npy", allow_pickle=True)
data_info = np.load("/content/features_from_lmdb/01243_info.npy", allow_pickle=True)

data_info.item()

data.shape

"""### <font color='Gold'><b> Number of 'unique' (based on file names) images </b></font>

"""

import pandas as pd
import os

annotation_dir = "/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations"
img_dir = "/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/"

# Collect all the annotations (from Phase-2)
train       = pd.read_json(f"{annotation_dir}/train.jsonl", lines=True)
dev_seen    = pd.read_json(f"{annotation_dir}/dev_seen.jsonl", lines=True)
dev_unseen  = pd.read_json(f"{annotation_dir}/dev_unseen.jsonl", lines=True)
test_seen   = pd.read_json(f"{annotation_dir}/test_seen.jsonl", lines=True)
test_unseen = pd.read_json(f"{annotation_dir}/test_unseen.jsonl", lines=True)

# Create 2 sets: 
#   A set of strings, 'a': for all the image names,
#   A set of lists, 'b': for all the image names in dataset, e.g. train, dev_seen, etc.
a = os.listdir(f"{img_dir}")
b = []
for i in [train, dev_seen, dev_unseen, test_seen, test_unseen]:
    b.append(list(i["img"].str.split("/").str.get(1)))

set_mapping = ['train', 'dev_seen', 'dev_unseen', 'test_seen', 'test_unseen']
total_size = 0
print("#of images in: ")
for idx, i in enumerate(b):
    total_size += len(set(i))
    print(f"\t'{set_mapping[idx]}'  \t:", len(set(i)))
else:
    print(f"\nIn total there are {total_size} images,",
          "\nBut the # of images in /img/ directory is: ", len(a))

# First, let's check if all the images are within jsonl files, in other words: 
# 'do we have an image in /img folder that's not in one of the .jsonl files?'
# 0 means every image in /img directory is in a jsonl file
print("#of images that are not in one of the .jsonl files: ", 
      len(set(a).symmetric_difference(set(b[0] + b[1] + b[2] + b[3] + b[4]))))

print("#of same images in between: ")
for i in range(0, 5):
    print("\n")
    for j in range(0, 5):
        if i != j:
            print(f"{set_mapping[i], set_mapping[j]}   \t: {len(set(b[i]) & set(b[j]))}")

"""As seen, `dev_seen.jsonl` and `dev_unseen.jsonl` have `400` same images. Let's double check that:"""

print(f"#of same images in {set_mapping[1], set_mapping[2]}: {len(set(b[1]) & set(b[2]))}",
      f"\n#of different images: {len(set(b[1]).symmetric_difference(set(b[2])))}")

"""That means in Phase-2, `100` images were removed from `dev_seen.jsonl` and `140` new images are added to the validation set.\
Hence;  `|dev_unseen.jsonl|=500-100+140=540`

### <font color='Gold'> <b> Image Feature Extraction </b> </font>

#### <font color='PaleGoldenrod'> <b> Discovering default image features from MMF </b> </font>

Download the features for Phase-2 which was published by MMF on 01.10.2020 [[source of link]](https://github.com/facebookresearch/mmf/blob/518a5a675586e4dc1b415a52a8a80c75edfc2960/mmf/configs/zoo/datasets.yaml#L232)
"""

# Download the features for Phase-2 from the following link
!wget https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz

"""Extract `features_2020_10_01.tar.gz`:
> this file actually extracts a folder `detectron.lmdb/` which
stores the `data.mdb` file, which is where all the image features are compressed into.
"""

!tar -xzf /content/features_2020_10_01.tar.gz

"""Extract image features from `detectron.lmdb/` folder to `/features/`.

**Note**
Interrupt the execution as we only want to have a sneek peek into a few features. So, no need to extract the whole image features.
"""

!python /content/mmf/tools/scripts/features/lmdb_conversion.py \
        --mode "extract" \
        --lmdb_path "/content/detectron.lmdb" \
        --features_folder "/content/features/" \

# Load only one image feature
import numpy as np
import os

img_list = os.listdir("/content/features/")
feat_dir = "/content/features/"


if len(img_list[0].split('_'))==2:
    img_id = img_list[0].split('_')[0]
else:
    img_id = img_list[0].split('_')[0].split('.')[0]

# There are 2 .npy files for each image, 
#   e.g. : for the image with 'image_id=75349':
#       - '75349.npy' : actual feature embedding
#       - '75349_info.npy' : meta-data about the image
data      = np.load(f"{feat_dir + img_id}.npy", allow_pickle=True)
data_info = np.load(f"{feat_dir + img_id}_info.npy", allow_pickle=True)

# Images are embedded to 2048 dimension!
# There are 100 bbox's
data.shape

# The meta-data about the image
data_info.item().keys()

print(f"image_id\t: {data_info.item()['image_id']}",
      f"\nnum_boxes\t: {data_info.item()['num_boxes']}",
      f"\nimage_height\t: {data_info.item()['image_height']}",
      f"\nimage_width\t: {data_info.item()['image_width']}",
      f"\nshape(bbox)\t: {data_info.item()['bbox'].shape}",
      f"\nshape(objects)\t: {data_info.item()['objects'].shape}",
      f"\nshape(cls_prob)\t: {data_info.item()['cls_prob'].shape}")

data_info.item()["objects"]

for i in [0, 1, 2]:
  print(f"max cls_prob of box #{[i]}: {data_info.item()['cls_prob'][i].max()}",
        f"\nindex of that class\t: {data_info.item()['cls_prob'][i].argmax()}\n",
        "-"*10)

data_info.item()["bbox"]

"""#### <font color='PaleGoldenrod'> <b> Different techniques for image feature extraction </b> </font>

##### <font color='PeachPuff'> <b> Extract image features using `Detectron2` & `ResNet-152` </b> </font>
"""

!python extract_region_feature.py

data = np.load("/content/features/hateful_memes/image_1.npy", allow_pickle=True)
data.item(0)

data.item(0)["features"].shape

data.item().keys()

"""##### <font color='PeachPuff'> <b> Extract image features using [`facebookresearch/grid-feats-vqa`](https://github.com/facebookresearch/grid-feats-vqa) </b> </font>"""

# Install required packages
!pip install -U git+https://github.com/facebookresearch/fvcore
!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git@ffff8ac'

!git clone https://github.com/vedanuj/grid-feats-vqa.git --branch region_features

cd grid-feats-vqa/

!python extract_region_feature.py \
              --config-file configs/X-152-region-c4.yaml \
              --dataset "hateful_memes" \
              --dataset-path "/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/" \

!ls /content/grid-feats-vqa/output/features/hateful_memes/

import numpy as np
data = np.load("/content/grid-feats-vqa/output/features/hateful_memes/01235.npy", allow_pickle=True)

data.item(0)

data.item(0)["bbox"].shape

"""##### <font color='PeachPuff'> <b> Extract image features using [`airsplay/py-bottom-up-attention`](https://github.com/airsplay/py-bottom-up-attention) </b> </font>

###### **Install packages**
"""

import os
os.chdir("/content/")
!git clone https://github.com/airsplay/py-bottom-up-attention.git
os.chdir("py-bottom-up-attention/")

!pip install -r requirements.txt

!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
# Install detectron2
!python setup.py build develop

"""###### **Extract!**"""

os.chdir("/content/hateful_memes/region_feature_extraction/")
!python extract.py

!ls /content/features/ | wc -l

"""--- 
## <font color='orange'> <b> Summary & FAQ </b> </font>

### <font color='Gold'> <b> Summary </b> </font>

#### <font color='Gold'> <b> Explain Like I'm 5 (ELI5) </b> </font>

### <font color='Gold'> <b> FAQ </b> </font>

*What is the size of the training data?*
> 

*Did you use additional dataset?*
> Yes. We used Memotion as an additional dataset

*Did you use pre-trained models?*
> Yes. We used `VisualBERT` which was pre-trained on `Masked Conceptual Captions` dataset, see <font color='magenta'> <b> IV. Fine-tuning pre-trained VisualBERT models on Hateful Memes </b> </font>. Then, the model was fine-tuned on the HM dataset. The pre-trained model is available from MMF: [See all the available pre-trained VisualBERT models from MMF](https://github.com/facebookresearch/mmf/tree/master/projects/pretrain_vl_right)

*Did you use default image features provided by MMF?*
> No. We extracted our own image features using Facebook's Detectron model, which uses ResNet-152 as its backbone. See <font color='magenta'> <b> III. Feature Extraction </b> </font> part in the notebook.

*What is the impact of `Majority Voting` technique in ROC-AUC score? And what do you think about the reason behind?*
>

**
>
"""